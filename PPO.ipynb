{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torch.autograd as autograd\n",
    "from torch.distributions import Categorical\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class critic(nn.Module):\n",
    "    def __init__(self,input_dim):\n",
    "        super(critic,self).__init__()\n",
    "        \n",
    "        # conv_layers\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # fc_layers\n",
    "        self.fc1 = nn.Linear(3136,512)\n",
    "        self.fc2 = nn.Linear(512,128)\n",
    "        self.fc3 = nn.Linear(128,1)\n",
    "\n",
    "    def forward(self,state):\n",
    "        # get batch_size\n",
    "        batch_size = state.shape[0]\n",
    "        \n",
    "        # forward conv layers\n",
    "        state = state.permute(0,3,1,2) #batch_size,通道數,長,寬\n",
    "        state = self.conv(state)\n",
    "        state = state.reshape(batch_size,-1) #flatten\n",
    "        \n",
    "        # forward fc layers\n",
    "        state = F.relu(self.fc1(state))\n",
    "        state = F.relu(self.fc2(state))\n",
    "        value = self.fc3(state)\n",
    "        \n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class actor(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(actor, self).__init__()\n",
    "        \n",
    "        # conv_layers\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # fc_layers\n",
    "        self.fc1 = nn.Linear(3136,512)\n",
    "        self.fc2 = nn.Linear(512,128)\n",
    "        self.fc3 = nn.Linear(128,output_dim)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        # get_batch_size\n",
    "        batch_size = state.shape[0]\n",
    "        \n",
    "        # forward conv_layers\n",
    "        state = state.permute(0,3,1,2) #batch_size,通道數,長,寬\n",
    "        state = self.conv(state)\n",
    "        state = state.reshape(batch_size,-1) #flatten\n",
    "        \n",
    "        # forward fc_layers\n",
    "        state = F.relu(self.fc1(state))\n",
    "        state = F.relu(self.fc2(state))\n",
    "        probs = F.softmax(self.fc3(state),dim=-1)\n",
    "        \n",
    "        return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOAgent:\n",
    "    def __init__(self,env,gamma = 0.99,clip = 0.2,lr = 1e-3,K_epoch = 4):\n",
    "        # commom\n",
    "        self.device = \"cuda\"\n",
    "        self.env = env\n",
    "        self.obs_dim = env.observation_space.shape\n",
    "        self.action_dim = env.action_space.n\n",
    "        \n",
    "        # Hyperparamters\n",
    "        self.gamma = gamma\n",
    "        self.clip = clip\n",
    "        self.lr = lr\n",
    "        self.K_epoch = K_epoch\n",
    "        \n",
    "        # critic\n",
    "        self.critic = critic(self.obs_dim).to(self.device)\n",
    "        self.critic.apply(self._weights_init)\n",
    "        \n",
    "        # actor_new\n",
    "        self.actor_new = actor(self.obs_dim,self.action_dim).to(self.device)\n",
    "        self.actor_new.apply(self._weights_init)\n",
    "        \n",
    "        # actor_old,sync with actor_new\n",
    "        self.actor_old = actor(self.obs_dim,self.action_dim).to(self.device)\n",
    "        self.sync()\n",
    "        \n",
    "        # optimizers\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(),lr=lr)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_new.parameters(),lr=lr)\n",
    "        \n",
    "        # recorder\n",
    "        self.recorder = {'a_loss':[],\n",
    "                         'v_loss':[],\n",
    "                         'e_loss':[],\n",
    "                         'ratio':[]}\n",
    "        \n",
    "        self.best_score = -np.inf\n",
    "            \n",
    "    @staticmethod\n",
    "    def _weights_init(m):\n",
    "        if hasattr(m,'weight'):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            nn.init.constant_(m.bias, 0.1)\n",
    "    \n",
    "    def sync(self):\n",
    "        for old_param, new_param in zip(self.actor_old.parameters(),self.actor_new.parameters()):\n",
    "            old_param.data.copy_(new_param.data)\n",
    "    \n",
    "    def get_action(self,state):\n",
    "        state = torch.FloatTensor(state).to(self.device)\n",
    "        probs = self.actor_new(state) # softmax_probs\n",
    "        dist = Categorical(probs) # Categorical distribution\n",
    "        act = dist.sample() # smaple action from this Categorical distribution\n",
    "        return act.item()\n",
    "    \n",
    "    def get_value(self,state):\n",
    "        state = torch.FloatTensor([state]).to(self.device)\n",
    "        value = self.critic(state)\n",
    "        return value.item()\n",
    "    \n",
    "    def compute_returns(self,rewards):\n",
    "        returns = []\n",
    "        G = 0\n",
    "        \n",
    "        for r in rewards[::-1]:\n",
    "            G = r + self.gamma*G\n",
    "            returns.insert(0,G)\n",
    "        \n",
    "        returns = np.array([i for i in returns]).ravel()\n",
    "        return torch.FloatTensor(returns).to(self.device).view(-1, 1)\n",
    "    \n",
    "    def normalize_img(self,x):\n",
    "        return x / 255.0\n",
    "    \n",
    "    def update(self,trajectory):\n",
    "        print('update...')\n",
    "        \n",
    "        # get trajectory\n",
    "        state = torch.FloatTensor([sars[0] for sars in trajectory]).to(self.device)\n",
    "        action = torch.LongTensor([sars[1] for sars in trajectory]).to(self.device).view(-1, 1)\n",
    "        rewards = [sars[2] for sars in trajectory]\n",
    "        next_state = torch.FloatTensor([sars[3] for sars in trajectory]).to(self.device)\n",
    "        done = torch.FloatTensor([sars[4] for sars in trajectory]).to(self.device).view(-1, 1)\n",
    "        \n",
    "        # update K_epoch\n",
    "        for _ in range(self.K_epoch):        \n",
    "            \n",
    "            # calculate critic loss\n",
    "            values = self.critic(state)\n",
    "            returns = self.compute_returns(rewards)\n",
    "            advantage = returns - values\n",
    "            critic_loss = (0.5*(advantage**2)).mean()\n",
    "            self.recorder['v_loss'].append(critic_loss.item())\n",
    "            \n",
    "            # calculate actor_loss\n",
    "            new_p = torch.gather(self.actor_new(state),1,action)\n",
    "            old_p = torch.gather(self.actor_old(state),1,action)\n",
    "            ratio = new_p / old_p\n",
    "            self.recorder['ratio'].append(ratio.mean().item())\n",
    "            \n",
    "            surr1 = ratio * advantage.detach()\n",
    "            surr2 = torch.clamp(ratio,1 - self.clip,1 + self.clip) * advantage.detach()\n",
    "            entropy_loss = Categorical(self.actor_new(state)).entropy().mean()\n",
    "            self.recorder['e_loss'].append(entropy_loss.item())\n",
    "            \n",
    "            actor_loss = -torch.min(surr1,surr2).mean() - 0.001*entropy_loss\n",
    "            self.recorder['a_loss'].append(actor_loss.item())\n",
    "            \n",
    "            # update critic\n",
    "            self.critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            self.critic_optimizer.step()\n",
    "            \n",
    "            # update actor\n",
    "            self.actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            self.actor_optimizer.step()\n",
    "        \n",
    "        # sync actor_old and actor_new\n",
    "        self.sync()\n",
    "        print('update ok!')\n",
    "    \n",
    "    def train(self,max_episodes):\n",
    "        episode_rewards = []\n",
    "        for episode in range(max_episodes):\n",
    "            \n",
    "            # initialize all\n",
    "            state = env.reset()\n",
    "            state = self.normalize_img(state)#預處理畫面\n",
    "            trajectory = []\n",
    "            episode_reward = 0\n",
    "            done = False\n",
    "            \n",
    "            # game loop\n",
    "            while not done:\n",
    "                env.render()\n",
    "                action = self.get_action([state])\n",
    "                next_state, reward, done, info = self.env.step(action)\n",
    "                reward,done = self.reward_fn(reward,done,info)\n",
    "                next_state = self.normalize_img(next_state)#預處理畫面\n",
    "                \n",
    "                # record\n",
    "                trajectory.append([state, action, reward, next_state, done])\n",
    "                episode_reward += reward\n",
    "                \n",
    "                # change state\n",
    "                state = next_state\n",
    "                    \n",
    "            # game over\n",
    "            print(\"Episode {} reward {} best {}\".format(episode,\n",
    "                                                        episode_reward,\n",
    "                                                        self.best_score))\n",
    "            \n",
    "            # 紀錄這一回合分數\n",
    "            episode_rewards.append(episode_reward)\n",
    "            \n",
    "            # 如果表現比歷史紀錄最好分數還差就更新agent\n",
    "            if episode_reward <= self.best_score:\n",
    "                self.update(trajectory) #update\n",
    "            \n",
    "            # 更新歷史最佳分數\n",
    "            self.best_score = max(self.best_score,episode_reward)\n",
    "            \n",
    "            # tensorboard update\n",
    "            try:\n",
    "                writer.add_scalar('v_loss',self.recorder['v_loss'][-1],episode)\n",
    "                writer.add_scalar('a_loss',self.recorder['a_loss'][-1],episode)\n",
    "                writer.add_scalar('e_loss',self.recorder['e_loss'][-1],episode)\n",
    "                writer.add_scalar('ratio',self.recorder['ratio'][-1],episode)\n",
    "                writer.add_scalar('score',episode_reward,episode)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        return episode_rewards\n",
    "    \n",
    "    def reward_fn(self,reward,done,info):\n",
    "        # if agent eat something\n",
    "        if info['status'] != 'small':\n",
    "            if info['status'] != self.last_state:\n",
    "                print('eat!')\n",
    "                reward += 100\n",
    "        \n",
    "        # if agent died\n",
    "        if info['life'] != 2:\n",
    "            print('died')\n",
    "            done = True\n",
    "        \n",
    "        # update agent's status\n",
    "        self.last_state = info['status']\n",
    "        return reward,done\n",
    "    \n",
    "    def play(self,max_episodes):\n",
    "        for episode in range(max_episodes):\n",
    "            # initialize all\n",
    "            state = env.reset()\n",
    "            state = self.normalize_img(state)#預處理畫面到區間[0,1]\n",
    "            episode_reward = 0\n",
    "            done = False\n",
    "            \n",
    "            # game loop\n",
    "            while not done:\n",
    "                self.env.render()\n",
    "                time.sleep(0.02)\n",
    "                action = self.get_action(state)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                next_state = self.normalize_img(next_state)#預處理畫面到區間[0,1]\n",
    "                episode_reward += reward\n",
    "                state = next_state\n",
    "            \n",
    "            # game over\n",
    "            print(\"Episode \" + str(episode) + \": \" + str(episode_reward))\n",
    "        \n",
    "        self.env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# env wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import cv2\n",
    "\n",
    "class MaxAndSkipEnv(gym.Wrapper):\n",
    "    def __init__(self, env, is_render, skip = 4):\n",
    "        # buffer空間只有(2,)\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self._obs_buffer = np.zeros((2,) + env.observation_space.shape, dtype=np.uint8)\n",
    "        self._skip = skip\n",
    "        self.is_render = is_render\n",
    "\n",
    "    def step(self, action):\n",
    "        #重複動作 k 次\n",
    "        total_reward = 0.0\n",
    "        done = None\n",
    "        for i in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            \n",
    "            if self.is_render:\n",
    "                self.env.render()\n",
    "            \n",
    "            if i == self._skip - 2:\n",
    "                self._obs_buffer[0] = obs\n",
    "            \n",
    "            if i == self._skip - 1:\n",
    "                self._obs_buffer[1] = obs\n",
    "            \n",
    "            total_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # 在時間維度上做 max pool\n",
    "        max_frame = self._obs_buffer.max(axis=0)\n",
    "        return max_frame, total_reward, done, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        return self.env.reset(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WarpFrame(gym.ObservationWrapper):\n",
    "    def __init__(self, env, width=84, height=84, grayscale=True, dict_space_key=None):\n",
    "        super().__init__(env)\n",
    "        self._width = width\n",
    "        self._height = height\n",
    "        self._grayscale = grayscale\n",
    "        self._key = dict_space_key\n",
    "        \n",
    "        if self._grayscale:\n",
    "            num_colors = 1\n",
    "        else:\n",
    "            num_colors = 3\n",
    "\n",
    "        new_space = gym.spaces.Box(\n",
    "            low=0,\n",
    "            high=255,\n",
    "            shape=(self._height, self._width, num_colors),\n",
    "            dtype=np.uint8)\n",
    "        \n",
    "        if self._key is None:\n",
    "            original_space = self.observation_space\n",
    "            self.observation_space = new_space\n",
    "        else:\n",
    "            original_space = self.observation_space.spaces[self._key]\n",
    "            self.observation_space.spaces[self._key] = new_space\n",
    "        \n",
    "        assert original_space.dtype == np.uint8 and len(original_space.shape) == 3\n",
    "\n",
    "    def observation(self, obs):\n",
    "        \n",
    "        if self._key is None:\n",
    "            frame = obs\n",
    "        else:\n",
    "            frame = obs[self._key]\n",
    "\n",
    "        if self._grayscale:\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "        \n",
    "        frame = cv2.resize(\n",
    "            frame, (self._width, self._height), interpolation=cv2.INTER_AREA)\n",
    "        \n",
    "        if self._grayscale:\n",
    "            frame = np.expand_dims(frame, -1)\n",
    "\n",
    "        if self._key is None:\n",
    "            obs = frame\n",
    "        else:\n",
    "            obs = obs.copy()\n",
    "            obs[self._key] = frame\n",
    "        \n",
    "        return obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 400, 'world': 1, 'x_pos': 40, 'x_pos_screen': 40, 'y_pos': 79}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "died\n",
      "Episode 0 reward 230.0 best -inf\n",
      "died\n",
      "Episode 1 reward 765.0 best 230.0\n",
      "died\n",
      "Episode 2 reward 814.0 best 765.0\n",
      "died\n",
      "Episode 3 reward 228.0 best 814.0\n",
      "update...\n",
      "update ok!\n",
      "died\n",
      "Episode 4 reward 197.0 best 814.0\n",
      "update...\n",
      "update ok!\n",
      "died\n",
      "Episode 5 reward 180.0 best 814.0\n",
      "update...\n",
      "update ok!\n",
      "died\n",
      "Episode 6 reward 507.0 best 814.0\n",
      "update...\n",
      "update ok!\n",
      "died\n",
      "Episode 7 reward 801.0 best 814.0\n",
      "update...\n",
      "update ok!\n",
      "died\n",
      "Episode 8 reward 854.0 best 814.0\n",
      "died\n",
      "Episode 9 reward 1239.0 best 854.0\n",
      "died\n",
      "Episode 10 reward 718.0 best 1239.0\n",
      "update...\n",
      "update ok!\n",
      "died\n",
      "Episode 11 reward 591.0 best 1239.0\n",
      "update...\n",
      "update ok!\n",
      "died\n",
      "Episode 12 reward 1164.0 best 1239.0\n",
      "update...\n",
      "update ok!\n",
      "died\n",
      "Episode 13 reward 1080.0 best 1239.0\n",
      "update...\n",
      "update ok!\n",
      "died\n",
      "Episode 14 reward 968.0 best 1239.0\n",
      "update...\n",
      "update ok!\n",
      "eat!\n",
      "died\n",
      "Episode 15 reward 1083.0 best 1239.0\n",
      "update...\n",
      "update ok!\n",
      "died\n",
      "Episode 16 reward 582.0 best 1239.0\n",
      "update...\n",
      "update ok!\n",
      "died\n",
      "Episode 17 reward 545.0 best 1239.0\n",
      "update...\n",
      "update ok!\n",
      "died\n",
      "Episode 18 reward 236.0 best 1239.0\n",
      "update...\n",
      "update ok!\n",
      "died\n",
      "Episode 19 reward 705.0 best 1239.0\n",
      "update...\n",
      "update ok!\n",
      "died\n",
      "Episode 20 reward 168.0 best 1239.0\n",
      "update...\n",
      "update ok!\n",
      "died\n",
      "Episode 21 reward 200.0 best 1239.0\n",
      "update...\n",
      "update ok!\n",
      "died\n",
      "Episode 22 reward 770.0 best 1239.0\n",
      "update...\n",
      "update ok!\n",
      "died\n",
      "Episode 23 reward 217.0 best 1239.0\n",
      "update...\n",
      "update ok!\n",
      "died\n",
      "Episode 24 reward 603.0 best 1239.0\n",
      "update...\n",
      "update ok!\n",
      "died\n",
      "Episode 25 reward 983.0 best 1239.0\n",
      "update...\n",
      "update ok!\n"
     ]
    }
   ],
   "source": [
    "from nes_py.wrappers import JoypadSpace\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT,COMPLEX_MOVEMENT\n",
    "\n",
    "# create env\n",
    "env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
    "env = JoypadSpace(env,SIMPLE_MOVEMENT)\n",
    "\n",
    "# wrapper env\n",
    "env = WarpFrame(env,grayscale=True)\n",
    "env = MaxAndSkipEnv(env,is_render=True,skip=4)\n",
    "\n",
    "# creaet agent\n",
    "agent = PPOAgent(env)\n",
    "\n",
    "# train\n",
    "history = agent.train(max_episodes=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "plt.plot(history)\n",
    "plt.plot(pd.Series(history).rolling(10).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.play(max_episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
